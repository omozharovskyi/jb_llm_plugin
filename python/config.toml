# Common setup
my_ip_url = "https://api.ipify.org"
log_level = "info"
llm_model = "tinyllama:latest"
# | Model Name                          | Description & Use Case                                                                             | Minimal Hardware Requirements                                |
# |-------------------------------------|----------------------------------------------------------------------------------------------------|----------------------------------------------------|
# | `codellama:7b-code`                 | Base Code Llama (7B) for code completion & infilling                                               | 16GB RAM; CPU OK, GPU accelerates (8GB)            |
# | `codellama:7b-python`               | Code Llama 7B fine-tuned on Python code                                                            | 16GB RAM min; GPU recommended (8GB+)               |
# | `codellama:7b-instruct`             | Instruction-tuned Code Llama 7B suitable for runnable code generation with natural-language prompt | 16GB RAM min; GPU recommended (8GB+)               |
# | `codellama:13b-code/python/instruct`| Code Llama 13B variants for enhanced code understanding & generation                               | 32GB RAM; strong CPU/GPU (e.g., RTX 3080+)         |
# | `codellama:34b-code/python/instruct`| Larger 34B variant, best for deep code generation & complex tasks                                  | 64GB RAM; high-end GPU like A100 recommended       |
# | `codellama:70b-code/python/instruct`| Top-tier 70B variant optimized for advanced code synthesis                                         | 128GB RAM; server GPUs (A100 40/80GB) or multi-GPU |
# | `starcoder2`                        | Open-code model for many languages (3B,7B,15B)                                                     | 32GB RAM; GPU advisable for 7B+                    |
# | `qwen2.5-coder`                     | Code-specific Qwen2.5 family (0.5B–32B), excels at code generation and reasoning                   | 8GB RAM for small; 64GB RAM + GPU for 7B+          |
retries = "10"
retry_interval = "10"
retry_timeout = "300"

[ssh]
user = "jbllm"
ssh_pub_key = "./sa-keys/jb-llm-plugin-ssh.pub"
ssh_secret_key = "./sa-keys/jb-llm-plugin-ssh"

[gcp]
project_name = "jb-llm-plugin"
sa_gcp_key = "./sa-keys/jb-llm-plugin-sa.json"
instance_name = "ollama-code-vm"
machine_type = "n1-standard-1"
# `machine_type` parameter defines the virtual machine (VM) type to be used when provisioning compute resources
# in Google Cloud Platform (GCP), such as with Compute Engine, Cloud Run, or Cloud Functions (2nd gen).
# machine_type = "n1-standard-1"
# This specifies the following:
# - Series: `n1-standard`
# - vCPUs: 1
# - Memory: 3.75 GB
# - Type: General-purpose virtual machine
# - Approx. price: ~$0.0475 per hour (in `us-central1` region)
# Choosing the appropriate machine type affects both cost and performance.
# GCP provides several machine families optimized for different workloads: general purpose, compute optimized, memory optimized, and GPU accelerated.
# | Series  | Purpose                     | Min Configuration           | Max Configuration              | Memory/vCPU | USD vCPU/hr |
# |---------|-----------------------------|-----------------------------|--------------------------------|-------------|-------------|
# | `e2`    | Gen.-purp., cost-optimized  | `e2-micro` (0.25 vCPU)      | `e2-highmem-16` (16 vCPU)      | 1–8 GB      | ~$0.033     |
# | `n1`    | General-purpose (legacy)    | `n1-standard-1` (1 vCPU)    | `n1-standard-96` (96 vCPU)     | 3.75 GB     | ~$0.0475    |
# | `n2`    | General-purpose, newer gen  | `n2-standard-2` (2 vCPU)    | `n2-highmem-128` (128 vCPU)    | 4–8 GB      | ~$0.052     |
# | `n2d`   | AMD-based general-purpose   | `n2d-standard-2` (2 vCPU)   | `n2d-highmem-224` (224 vCPU)   | 4–8 GB      | ~$0.041     |
# | `c2`    | Compute-optimized (Intel)   | `c2-standard-4` (4 vCPU)    | `c2-standard-60` (60 vCPU)     | 4 GB        | ~$0.063     |
# | `c3`    | Comp.-optim. (Intel/AMD)    | `c3-standard-4` (4 vCPU)    | `c3-standard-176` (176 vCPU)   | 4 GB        | ~$0.069     |
# | `m1/m2` | Memory-optimized            | `m1-megamem-96` (96 vCPU)   | `m2-ultramem-208` (208 vCPU)   | upto 28.9 GB| ~$0.228     |
# | `a2`    | Accelerator-optimized (GPU) | `a2-highgpu-1g` (1vCPU+1GPU)| `a2-megagpu-16g` (96vCPU+16GPU)| 85–96 GB    | ~$0.113+GPU |
# | `t2a`   | ARM-based (Ampere Altra)    | `t2a-standard-1` (1 vCPU)   | `t2a-standard-48` (48 vCPU)    | 4 GB        | ~$0.021     |
image_family = "ubuntu-2204-lts"
hdd_size = "10" # for large model specify not less than 50 (Gb)
gpu_accelerator = "nvidia-tesla-t4"
# ============================================================================
# Available NVIDIA GPUs in GCP (as for 01.07.2025)
# ============================================================================
# | Model                                | USD/hour | Architecture | Purpose                                   |
# |--------------------------------------|----------|--------------|-------------------------------------------|
# | nvidia-tesla-t4                      | 0.35     | Turing       | ML inference, general GPU tasks           |
# | nvidia-t4-virtual-workstation        | 0.55     | Turing       | Graphics workloads, virtual workstation   |
# | nvidia-l4                            | 0.71     | Ada Lovelace | AI inference, video analytics             |
# | nvidia-tesla-p4                      | 0.60     | Pascal       | ML inference, video processing            |
# | nvidia-tesla-p4-virtual-workstation  | 0.80     | Pascal       | Graphics workstation, light rendering     |
# | nvidia-tesla-p100                    | 1.46     | Pascal       | Training ML models, HPC                   |
# | nvidia-tesla-p100-virtual-workstation| 1.66     | Pascal       | Graphics workstation, compute workloads   |
# | nvidia-tesla-v100                    | 2.48     | Volta        | Training ML, HPC, general purpose compute |
# | nvidia-tesla-h100-80gb               | 6.98     | Hopper       | State-of-the-art ML/AI, LLM training      |
# | nvidia-a100-40gb                     | 3.67     | Ampere       | Large model training, HPC                 |
# | nvidia-a100-80gb                     | 6.25     | Ampere       | Very large ML training, HPC               |
# ============================================================================
firewall_tag = "ollama-server"
zone_priority = "europe,us,*,asia"
firewall_rule_name = "allow-ollama-api-from-my-ip"
